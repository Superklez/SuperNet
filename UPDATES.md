Hello, Joseph Herrera (a.k.a. Superklez) here! 

November 21, 2020
Right now, I am working on implementing L2 regularization to SuperNet. It's been tough, I've been
getting many errors when trying to debug it but I realized that SuperNet is VERY SUPER DUPER TO
THE MAX sensitive to the choice of hyperparameters. This could be because of my choice of parameter
optimization (updating the weights). Nothing to worry, I'm sure I can make it better soon! After
I implement L2 regularization, I will attempt to implement dropout regularization, but I will not
pull it to the main branch yet (not until I know how to and do implement the Adam optimizer,
hopefully this makes SuperNet more robust).
